{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уменьшим размер словаря для лучшей сходимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(binary=True, min_df=25,\n",
       "                stop_words=frozenset({'a', 'about', 'above', 'across', 'after',\n",
       "                                      'afterwards', 'again', 'against', 'all',\n",
       "                                      'almost', 'alone', 'along', 'already',\n",
       "                                      'also', 'although', 'always', 'am',\n",
       "                                      'among', 'amongst', 'amoungst', 'amount',\n",
       "                                      'an', 'and', 'another', 'any', 'anyhow',\n",
       "                                      'anyone', 'anything', 'anyway',\n",
       "                                      'anywhere', ...}))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS,\n",
    "                             analyzer='word', binary=True, min_df = 25)\n",
    "vectorizer.fit(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4983"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сэмплируем из данного распределения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_weight(weights):      # Возвращает позицию относительно веса\n",
    "    weights_normed = np.sort(weights) / np.sum(weights)\n",
    "    weights_bounded = np.cumsum(weights_normed)\n",
    "    rand = np.random.rand()\n",
    "    for i in range(len(weights)):\n",
    "        if(rand < weights_bounded[i]):\n",
    "            rand = np.argsort(weights)[i]\n",
    "            break;\n",
    "    return rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_topic = np.zeros(len(vectorizer.vocabulary_), dtype = int) # z (к какой теме относится слово)\n",
    "num_topic = np.zeros(len(newsgroups_train.target_names))           # Счетчик n_k\n",
    "num_topic_word = np.zeros((len(newsgroups_train.target_names), len(vectorizer.vocabulary_)))   # Счетчик n_k,w\n",
    "num_text_topic = np.zeros((len(newsgroups_train.data), len(newsgroups_train.target_names)))    # Счетчик n_d,k\n",
    "alpha = np.zeros(len(newsgroups_train.target_names))               # Распределение тем по текстам\n",
    "beta = np.zeros((len(newsgroups_train.target_names), len(vectorizer.vocabulary_)))  # Распределение тем по словам\n",
    "\n",
    "for i in range(len(vectorizer.vocabulary_)):\n",
    "    word_to_topic[i] = generate_with_weight(np.full(20, 1/20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обновляем счётчики\n",
    "for i in range(len(newsgroups_train.data)):\n",
    "    alpha[newsgroups_train.target[i]] = alpha[newsgroups_train.target[i]] + 1\n",
    "    text = newsgroups_train.data[i]\n",
    "    beta[newsgroups_train.target[i]] = beta[newsgroups_train.target[i]] + vectorizer.transform([text])\n",
    "    x = np.resize(vectorizer.transform([text]).toarray(), len(vectorizer.vocabulary_))\n",
    "    b = np.argwhere(x)\n",
    "    c = word_to_topic[b]\n",
    "    for j in range(len(num_topic)):\n",
    "        num_text_topic[i, j] = len(c[(c == j)])\n",
    "        num_topic[j] = num_topic[j] + len(c[(c == j)])\n",
    "    text_transformed = vectorizer.inverse_transform(vectorizer.transform([text]))[0]\n",
    "    for j in range(len(text_transformed)):\n",
    "        word = vectorizer.vocabulary_.get(text_transformed[j])\n",
    "        num_topic_word[word_to_topic[word], word] = num_topic_word[word_to_topic[word], word] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count in range(50):                            # Для устойчивости делаем несколько раз \n",
    "    for i in range(len(newsgroups_train.data)):\n",
    "        text = newsgroups_train.data[i]\n",
    "        text_transformed = vectorizer.inverse_transform(vectorizer.transform([text]))[0]\n",
    "        for j in range(len(text_transformed)):             # Меняем счетчики\n",
    "            word = vectorizer.vocabulary_.get(text_transformed[j])     # Индекс слова в словаре\n",
    "            topic = word_to_topic[word]\n",
    "            num_text_topic[i, topic] = num_text_topic[i, topic] - 1\n",
    "            num_topic[topic] = num_topic[topic] - 1\n",
    "            num_topic_word[topic, word] = num_topic_word[topic, word] - 1\n",
    "\n",
    "            p = np.zeros(len(num_topic))\n",
    "            for k in range(len(num_topic)):\n",
    "                p[k] = (num_text_topic[i, k] + alpha[k]) * (num_topic_word[k, word] + beta[k, word]) / (num_topic[k] + np.sum(beta[k]))\n",
    "            topic = generate_with_weight(np.abs(p))\n",
    "            word_to_topic[word] = topic\n",
    "            num_text_topic[i, topic] = num_text_topic[i, topic] + 1\n",
    "            num_topic[topic] = num_topic[topic] + 1\n",
    "            num_topic_word[topic, word] = num_topic_word[topic, word] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Top 10 words in the Topic = 1\n",
      "\n",
      "long thing mail great god ago means comes man tried \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 2\n",
      "\n",
      "place example today including message major disk early size thinking \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 3\n",
      "\n",
      "think got old second following run computer large source told \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 4\n",
      "\n",
      "fact lot list note running deal started body bible process \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 5\n",
      "\n",
      "little trying works change short went involved considered purpose screen \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 6\n",
      "\n",
      "does work year looking times news similar wouldn evidence rights \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 7\n",
      "\n",
      "yes doing getting kind 30 file article cost national level \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 8\n",
      "\n",
      "come day non cause certain past fast friend half 21 \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 9\n",
      "\n",
      "people make ll mean problems small instead matter simple especially \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 10\n",
      "\n",
      "problem possible idea software experience offer box various dead groups \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 11\n",
      "\n",
      "way didn 10 information start 25 area haven add months \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 12\n",
      "\n",
      "ve post called support drive big car working access took \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 13\n",
      "\n",
      "new things email pretty nice net live consider check wanted \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 14\n",
      "\n",
      "time end group free ask key 18 assume death bought \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 15\n",
      "\n",
      "help wrong state given university files write words children opinion \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 16\n",
      "\n",
      "using said 20 says card version play posted force programs \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 17\n",
      "\n",
      "edu far hard saying certainly common care turn self happened \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 18\n",
      "\n",
      "believe best true available away feel making looks save shows \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 19\n",
      "\n",
      "going bad life hi understand american oh position special color \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = 20\n",
      "\n",
      "want let course left buy known advance couple 24 window \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Топ-10 слов по каждому тегу\n",
    "inverse_dict = {v:k  for k,v in vectorizer.vocabulary_.items()}\n",
    "for i in range(len(newsgroups_train.target_names)):\n",
    "    #print('    Top 10 words in the Topic = {0}'.format(newsgroups_train.target_names[i]))\n",
    "    print('    Top 10 words in the Topic = '+str(i+1))\n",
    "    print()\n",
    "    x = np.argsort(num_topic_word[i]) [word_to_topic[np.argsort(num_topic_word[i])] == i] [:-11:-1]\n",
    "    for j in range(len(x)):\n",
    "        print(inverse_dict.get(x[j]), end = ' ')\n",
    "    print()\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Изначальные топики\n",
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что можно соотнести некоторые изначальные топики с тэгами алгоритма.\n",
    "Например:\n",
    "Topic = 12 - 'rec.autos'\n",
    "Topic = 10 - 'comp.os.ms-windows.misc'\n",
    "Topic = 13 - 'comp.windows.x'\n",
    "Topic = 19 - 'talk.politics.mideast'\n",
    "Topic = 3 - 'sci.electronics'\n",
    "Topic = 7 - 'talk.politics.misc'\n",
    "Topic = 15 - 'sci.med'\n",
    "Topic = 4 - 'talk.religion.misc'\n",
    "Topic = 18 - 'alt.atheism'\n",
    "Topic = 2 - 'comp.sys.ibm.pc.hardware'\n",
    "Topic = 5 - 'comp.graphics'\n",
    "Однако, с другой стороны, замечаем, что разбиение происходит лишь частично, что говорит о необходимости большего числа итераций."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
