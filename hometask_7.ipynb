{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уменьшим размер словаря для лучшей сходимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(binary=True, min_df=25,\n",
       "                stop_words=frozenset({'a', 'about', 'above', 'across', 'after',\n",
       "                                      'afterwards', 'again', 'against', 'all',\n",
       "                                      'almost', 'alone', 'along', 'already',\n",
       "                                      'also', 'although', 'always', 'am',\n",
       "                                      'among', 'amongst', 'amoungst', 'amount',\n",
       "                                      'an', 'and', 'another', 'any', 'anyhow',\n",
       "                                      'anyone', 'anything', 'anyway',\n",
       "                                      'anywhere', ...}))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS,\n",
    "                             analyzer='word', binary=True, min_df = 25)\n",
    "vectorizer.fit(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4983"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сэмплируем из данного распределения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_weight(weights):      # Возвращает позицию относительно веса\n",
    "    weights_normed = np.sort(weights) / np.sum(weights)\n",
    "    weights_bounded = np.cumsum(weights_normed)\n",
    "    rand = np.random.rand()\n",
    "    for i in range(len(weights)):\n",
    "        if(rand < weights_bounded[i]):\n",
    "            rand = np.argsort(weights)[i]\n",
    "            break;\n",
    "    return rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_topic = np.zeros(len(vectorizer.vocabulary_), dtype = int) # z (к какой теме относится слово)\n",
    "num_topic = np.zeros(len(newsgroups_train.target_names))           # Счетчик n_k\n",
    "num_topic_word = np.zeros((len(newsgroups_train.target_names), len(vectorizer.vocabulary_)))   # Счетчик n_k,w\n",
    "num_text_topic = np.zeros((len(newsgroups_train.data), len(newsgroups_train.target_names)))    # Счетчик n_d,k\n",
    "alpha = np.zeros(len(newsgroups_train.target_names))               # Распределение тем по текстам\n",
    "beta = np.zeros((len(newsgroups_train.target_names), len(vectorizer.vocabulary_)))  # Распределение тем по словам\n",
    "\n",
    "for i in range(len(vectorizer.vocabulary_)):\n",
    "    word_to_topic[i] = generate_with_weight(np.full(20, 1/20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обновляем счётчики\n",
    "for i in range(len(newsgroups_train.data)):\n",
    "    alpha[newsgroups_train.target[i]] = alpha[newsgroups_train.target[i]] + 1\n",
    "    text = newsgroups_train.data[i]\n",
    "    beta[newsgroups_train.target[i]] = beta[newsgroups_train.target[i]] + vectorizer.transform([text])\n",
    "    x = np.resize(vectorizer.transform([text]).toarray(), len(vectorizer.vocabulary_))\n",
    "    b = np.argwhere(x)\n",
    "    c = word_to_topic[b]\n",
    "    for j in range(len(num_topic)):\n",
    "        num_text_topic[i, j] = len(c[(c == j)])\n",
    "        num_topic[j] = num_topic[j] + len(c[(c == j)])\n",
    "    text_transformed = vectorizer.inverse_transform(vectorizer.transform([text]))[0]\n",
    "    for j in range(len(text_transformed)):\n",
    "        word = vectorizer.vocabulary_.get(text_transformed[j])\n",
    "        num_topic_word[word_to_topic[word], word] = num_topic_word[word_to_topic[word], word] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count in range(5):                            # Для устойчивости делаем несколько раз \n",
    "    for i in range(len(newsgroups_train.data)):\n",
    "        text = newsgroups_train.data[i]\n",
    "        text_transformed = vectorizer.inverse_transform(vectorizer.transform([text]))[0]\n",
    "        for j in range(len(text_transformed)):             # Меняем счетчики\n",
    "            word = vectorizer.vocabulary_.get(text_transformed[j])     # Индекс слова в словаре\n",
    "            topic = word_to_topic[word]\n",
    "            num_text_topic[i, topic] = num_text_topic[i, topic] - 1\n",
    "            num_topic[topic] = num_topic[topic] - 1\n",
    "            num_topic_word[topic, word] = num_topic_word[topic, word] - 1\n",
    "\n",
    "            p = np.zeros(len(num_topic))\n",
    "            for k in range(len(num_topic)):\n",
    "                p[k] = (num_text_topic[i, k] + alpha[k]) * (num_topic_word[k, word] + beta[k, word]) / (num_topic[k] + np.sum(beta[k]))\n",
    "            topic = generate_with_weight(np.abs(p))\n",
    "            word_to_topic[word] = topic\n",
    "            num_text_topic[i, topic] = num_text_topic[i, topic] + 1\n",
    "            num_topic[topic] = num_topic[topic] + 1\n",
    "            num_topic_word[topic, word] = num_topic_word[topic, word] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Top 10 words in the Topic = alt.atheism\n",
      "\n",
      "old based person makes works large today running net advance \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = comp.graphics\n",
      "\n",
      "really post support file space original hear children lines center \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = comp.os.ms-windows.misc\n",
      "\n",
      "time want come group email free control agree instead buy \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = comp.sys.ibm.pc.hardware\n",
      "\n",
      "comes posting files fast 93 future coming office dead players \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = comp.sys.mac.hardware\n",
      "\n",
      "way doesn hard questions making john single pay 21 build \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = comp.windows.x\n",
      "\n",
      "hope computer feel open clear sense asked ones sale sun \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = misc.forsale\n",
      "\n",
      "tell left team evidence numbers friend 19 sent bought text \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = rec.autos\n",
      "\n",
      "sure long line 15 public money low pc claim sell \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = rec.motorcycles\n",
      "\n",
      "used ll life start game version certainly nice home posted \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = rec.sport.baseball\n",
      "\n",
      "ve software tried play include assume argument company strong supposed \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = rec.sport.hockey\n",
      "\n",
      "10 number non key unless told early april close mentioned \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = sci.crypt\n",
      "\n",
      "year little fact drive understand fine offer allow situation body \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = sci.electronics\n",
      "\n",
      "thanks years available quite 20 heard including experience saw includes \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = sci.med\n",
      "\n",
      "possible 30 remember current type cost memory soon 000 board \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = sci.space\n",
      "\n",
      "does point information day given ago says goes correct difference \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = soc.religion.christian\n",
      "\n",
      "thing course true second government getting state note card answer \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = talk.politics.guns\n",
      "\n",
      "let question end following looking doing problems message went mark \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = talk.politics.mideast\n",
      "\n",
      "edu said reason 14 news guess care disk view hold \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = talk.politics.misc\n",
      "\n",
      "probably com local haven 18 self programs death war internet \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = talk.religion.misc\n",
      "\n",
      "great set 12 months needed willing outside weeks unfortunately higher \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Топ-10 слов по каждому тегу по алгоритму\n",
    "\n",
    "inverse_dict = {v:k  for k,v in vectorizer.vocabulary_.items()}\n",
    "for i in range(len(newsgroups_train.target_names)):\n",
    "    print('    Top 10 words in the Topic = {0}'.format(newsgroups_train.target_names[i]))\n",
    "    print()\n",
    "    x = np.argsort(num_topic_word[i]) [word_to_topic[np.argsort(num_topic_word[i])] == i] [:-11:-1]\n",
    "    for j in range(len(x)):\n",
    "        print(inverse_dict.get(x[j]), end = ' ')\n",
    "    print()\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Top 10 words in the Topic = alt.atheism\n",
      "\n",
      "say atheism makes actually person based islam away book christian \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = comp.graphics\n",
      "\n",
      "graphics file run post appreciated vga support lines original programming \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = comp.os.ms-windows.misc\n",
      "\n",
      "know using dos want hi time microsoft got anybody group \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = comp.sys.ibm.pc.hardware\n",
      "\n",
      "speed 16 data port right uses fast vlb cache screen \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = comp.sys.mac.hardware\n",
      "\n",
      "apple video scsi way hard help doesn lc machine powerbook \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = comp.windows.x\n",
      "\n",
      "window windows code program sun x11r5 look mail send xlib \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = misc.forsale\n",
      "\n",
      "shipping price use brand contact case address obo 250 trade \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = rec.autos\n",
      "\n",
      "engine problem better dealer sure long bad oil honda model \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = rec.motorcycles\n",
      "\n",
      "bike ride going ll motorcycle used road work miles rider \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = rec.sport.baseball\n",
      "\n",
      "baseball season hit did play ve player people ball fan \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = rec.sport.hockey\n",
      "\n",
      "hockey think don games nhl teams playoffs rangers cup 10 \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = sci.crypt\n",
      "\n",
      "encryption chip bit algorithm escrow privacy fact means real private \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = sci.electronics\n",
      "\n",
      "thanks need circuit high years amp series chips 20 available \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = sci.med\n",
      "\n",
      "just soon banks skepticism chastity surrender doctor treatment medicine read \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = sci.space\n",
      "\n",
      "new does information station spacecraft didn thought days day point \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = soc.religion.christian\n",
      "\n",
      "jesus true word course wrong love religion thing heaven words \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = talk.politics.guns\n",
      "\n",
      "make weapons let federal weapon killed question batf waco states \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = talk.politics.mideast\n",
      "\n",
      "like said turks military turkey occupied area taken mr israelis \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = talk.politics.misc\n",
      "\n",
      "clinton president tax congress rights political man probably war com \n",
      "\n",
      "\n",
      "\n",
      "    Top 10 words in the Topic = talk.religion.misc\n",
      "\n",
      "god good things christ try set great koresh kent law \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Топ-10 слов по каждому тегу из датасета\n",
    "\n",
    "for i in range(len(newsgroups_train.target_names)):\n",
    "    print('    Top 10 words in the Topic = {0}\\n'.format(newsgroups_train.target_names[i]))\n",
    "    x = np.argsort(beta[i]) [word_to_topic[np.argsort(beta[i])] == i] [:-11:-1]\n",
    "    for j in range(len(x)):\n",
    "        print(inverse_dict.get(x[j]), end = ' ')\n",
    "    print()\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм достаточно быстро ставит в соответствие каждому слову определенный тэг, который на самом деле соответствует основной тематике слова. Однако проведенных итераций явно недостотаточно для получения распределения тэгов над словами, удовлетворящего критерию стабильности и представлению о смысле слов. Это происходит потому, что в алгоритме на каждом шаге счётчики меняются на 1, в то время как объём слов помноженный на количество тэгов очень велик. Таким образом, для получения осмысленных результатов для распределения тэгов над словами необходимо провести намного больше итераций."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
